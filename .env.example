# =============================================================================
# LLM CrossCheck AI Service - Environment Configuration
# =============================================================================

# Application Settings
DEBUG=true
ENVIRONMENT=development
LOG_LEVEL=INFO

# Database Configuration
MONGODB_URL=mongodb://admin:password@localhost:27017/llm_crosscheck?authSource=admin
REDIS_URL=redis://localhost:6379/0

# LLM Provider API Keys (Required)
OPENAI_API_KEY=sk-your-openai-api-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# LLM Configerations
DEFAULT_MODEL=gpt-4o
MAX_TOKENS=4000
TEMPERATURE=0.7

# Optional LLM Provider API Keys
MISTRAL_API_KEY=your-mistral-api-key-here
COHERE_API_KEY=your-cohere-api-key-here
HUGGING_FACE_API_KEY=hf_your-hugging-face-token-here

# Google AI (if using Google)
GOOGLE_API_KEY=your-google-ai-api-key

# Rate Limiting
DEFAULT_MAX_REQUESTS_PER_MINUTE=60
DEFAULT_MAX_TOKENS_PER_MINUTE=150000

# Template Configuration
PROMPT_TEMPLATE_DIR=prompts
AUTO_RELOAD_TEMPLATES=true
TEMPLATE_CACHE_SIZE=50

# Application Configuration
APP_NAME=LLM CrossCheck AI Service
APP_VERSION=0.1.0
APP_DESCRIPTION=A robust Python service for cross-checking and validating Large Language Model outputs
DEBUG=false
ENVIRONMENT=development

# Server Configuration
HOST=0.0.0.0
PORT=8000
WORKERS=4
RELOAD=false

# Redis Configuration
REDIS_URL=redis://localhost:6379/0
REDIS_PASSWORD=
REDIS_DB=0

# Authentication & Security
SECRET_KEY=your-secret-key-change-this-in-production
ACCESS_TOKEN_EXPIRE_MINUTES=30
ALGORITHM=HS256

# Logging Configuration
LOG_LEVEL=INFO
LOG_FORMAT=json
LOG_FILE=logs/app.log

# Monitoring Configuration
PROMETHEUS_PORT=9090
ENABLE_METRICS=true

# Celery Configuration (for background tasks)
CELERY_BROKER_URL=redis://localhost:6379/1
CELERY_RESULT_BACKEND=redis://localhost:6379/2

# CORS Configuration
ALLOWED_ORIGINS=["http://localhost:3000", "http://localhost:8080"]
ALLOWED_METHODS=["GET", "POST", "PUT", "DELETE"]
ALLOWED_HEADERS=["*"]
